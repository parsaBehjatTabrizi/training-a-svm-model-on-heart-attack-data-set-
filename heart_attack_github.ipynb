{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mQIN_GuaanAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#heart attack predictor.py\n",
        "\n",
        "---\n",
        "\n",
        "**Support Vector Machine (SVM) Model**\n",
        "\n",
        "Support Vector Machine (SVM) is a classification algorithm that works by finding a decision boundary (hyperplane) that best separates data points into different classes. It’s particularly effective for datasets with clear margins between classes and can be tuned with hyperparameters such as the kernel type, regularization (C), and gamma for better performance. In this project, SVM is used to classify whether a patient is at high or low risk of a heart attack.\n",
        "\n",
        "---\n",
        "**Importance of Heart Attack Risk Prediction**\n",
        "\n",
        "Heart attacks are one of the leading causes of death globally, making their early detection crucial. This project uses machine learning to predict heart attack risks based on patient data, such as age, blood pressure, and cholesterol levels. Such a system can provide valuable insights to healthcare providers, enabling timely medical intervention and improving patient outcomes.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Import Necessary Libraries**\n",
        "\n",
        "\n",
        "This part loads essential libraries like NumPy and Pandas for data handling, Scikit-learn for preprocessing, model building, and evaluation, and Joblib for saving the trained model. These tools form the foundation for building and evaluating the SVM model.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "tnIFSvg5d6cE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdfV2Tmxdqqo"
      },
      "outputs": [],
      "source": [
        "### Heart Attack Risk Predictor Jupyter Notebook\n",
        "\n",
        "# Import Necessary Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I upload my dataset on google drive and then give the access to the code but you can download the data set on your computer and then give access directly by this code below"
      ],
      "metadata": {
        "id": "gM9d3cwTeOTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with open('/content/drive/MyDrive/Colab Notebooks/your_file.py', 'r') as f:\n",
        " #file_content = f.read()"
      ],
      "metadata": {
        "id": "ISuuLZPFeGfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accessing the google drive\n",
        "from google.colab import drive\n",
        "drive.mount ('/content/drive')"
      ],
      "metadata": {
        "id": "rcm4HazdeSEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this step i mounts my Google Drive in the Colab environment, allowing you to access datasets stored in the cloud directly from the notebook. This is useful for large datasets or when collaborating with others.\n",
        "\n",
        "---\n",
        "\n",
        "Don’t forget to put your folder name and address in parentheses"
      ],
      "metadata": {
        "id": "NYwKBaFaeYky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#locating the dataset file\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/my doc/your file name.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jQD96TFheWQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Dataset**\n",
        "---\n",
        "The dataset is loaded using Pandas into a DataFrame. A preview of the data (df.head()) ensures that it has been correctly loaded, providing insight into its structure and column names."
      ],
      "metadata": {
        "id": "1cRsW432fyOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (assuming it's in a CSV file named 'heart_attack_risk.csv')\n",
        "data = pd.read_csv('/content/drive/MyDrive/my doc/heart orginal.csv')"
      ],
      "metadata": {
        "id": "gViUAvl-eeMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the Dataset**\n",
        "---\n",
        "The data is split into features (X) and target labels (y), followed by a division into training and testing sets. This step ensures that the model can learn patterns from one part of the data (training) and be evaluated on unseen data (testing)."
      ],
      "metadata": {
        "id": "J7ZTWIcHf4eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "PzeSXOt-egdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the SVM Pipeline**\n",
        "---\n",
        "A pipeline is created that standardizes the features using StandardScaler and applies the SVM model. The pipeline simplifies the workflow by chaining preprocessing and modeling steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "qBEC5fe5gBzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline with preprocessing and model\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(random_state=42))\n",
        "    ])"
      ],
      "metadata": {
        "id": "j8UjC9qZekZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Hyperparameters for Grid Search**\n",
        "---\n",
        "Grid search is used to test different hyperparameter combinations for the SVM model. This ensures that the model achieves the best performance by fine-tuning parameters like the regularization strength (C), kernel type, and gamma."
      ],
      "metadata": {
        "id": "ggFHEJO7gIxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for grid search\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],\n",
        "    'svm__kernel': ['rbf', 'linear'],\n",
        "    'svm__gamma': ['scale', 'auto', 0.1, 0.01, 0.001]\n",
        "}"
      ],
      "metadata": {
        "id": "F-yR8JhJelKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Model with Grid Search**\n",
        "---\n",
        "The model is trained using the best combination of hyperparameters identified by grid search. This step involves fitting the pipeline to the training data and selecting the most optimal configuration."
      ],
      "metadata": {
        "id": "WuBYNOslgVMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "MbfCcvxMelXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "k7zonTZAeroi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "ssU-eBa8ewqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "xkl8oa07ezLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict heart attack risk for new data\n",
        "def predict_heart_attack_risk(new_data):\n",
        "    return best_model.predict(new_data)\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "lbUFOAxlezqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Saving the Trained Model**\n",
        "---\n",
        "The model is saved using Joblib for reuse. This ensures the model does not need to be retrained every time and can be easily loaded for predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "c5OX4lvygjzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model using joblib\n",
        "joblib.dump(best_model, 'heart_attack_model.pkl')  # Save with .pkl extension\n",
        "# Load the trained model\n",
        "model = joblib.load('heart_attack_model.pkl') # load the model\n",
        "# Load the trained model (make sure you've run the training code and saved the model first)\n",
        "model = joblib.load('heart_attack_model.pkl')"
      ],
      "metadata": {
        "id": "c4iGT5Haez5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Input Fields**\n",
        "---\n",
        "Interactive input fields are created using widgets, allowing users to enter new patient data directly into the notebook. This feature makes the project accessible to non-technical users.\n",
        "\n"
      ],
      "metadata": {
        "id": "x9g9oUcDgquy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input fields\n",
        "fields = [\n",
        "    \"Age\", \"Sex (0:Female, 1:Male)\", \"Chest Pain Type (0-3)\",\n",
        "    \"Resting Blood Pressure (mm Hg)\", \"Alcohol Drink (0:No, 1:Yes)\",\n",
        "    \"Fasting Blood Sugar (0:No, 1:Yes)\", \"Resting ECG Results (0-2)\",\n",
        "    \"Max Heart Rate\", \"Exercise Induced Angina (0:No, 1:Yes)\",\n",
        "    \"Previous Peak\", \"Slope (0-2)\", \"Number of Major Vessels (0-3)\",\n",
        "    \"Thalassemia (0-3)\"\n",
        "]"
      ],
      "metadata": {
        "id": "PCTS1n4pe4Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making Predictions**\n",
        "---\n",
        "The final step involves using the saved model to predict the heart attack risk based on the user’s inputs. The results are displayed interactively, indicating whether the patient is at high or low risk.\n",
        "\n"
      ],
      "metadata": {
        "id": "ItooD7Ykgykt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input widgets\n",
        "inputs = {field: widgets.FloatText(description=field) for field in fields}\n",
        "\n",
        "# Create a button widget\n",
        "button = widgets.Button(description=\"Predict\")\n",
        "output = widgets.Output()\n",
        "\n",
        "# Display widgets\n",
        "form = widgets.VBox(list(inputs.values()) + [button, output])\n",
        "display(form)\n"
      ],
      "metadata": {
        "id": "-sX5WlNWe5bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end i made a mini app to get thet valuables from user and put it into the svm model and then show the result"
      ],
      "metadata": {
        "id": "vObkoRjyfBam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make prediction\n",
        "def predict(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        try:\n",
        "            input_data = np.array([[inputs[field].value for field in fields]])\n",
        "            prediction = model.predict(input_data)\n",
        "            result = \"High Risk\" if prediction[0] == 1 else \"Low Risk\"\n",
        "            display(HTML(f\"<h3>Heart Attack Risk: {result}</h3>\"))\n",
        "        except ValueError:\n",
        "            display(HTML(\"<h3 style='color: red;'>Error: Please enter valid numeric values for all fields.</h3>\"))\n",
        "\n",
        "# Attach the function to the button\n",
        "button.on_click(predict)"
      ],
      "metadata": {
        "id": "NLpfWgtkfE7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}